---
title: 'P&S-2025: Lab assignment 3'
author: "Sofiia Trush, Taras Kopach, Daryna Onopriienko"
output:
  html_document:
    df_print: paged
---

**Work breakdown**

-   Task 1: Taras Kopach

-   Task 2: Sofiia Trush

-   Task 3: Daryna Onopriienko

## Task 1

The expected value of the exponential distribution E(λ) is 1/λ, so that a good point estimate of the parameter θ := 1/λ is the sample mean $\overline{X}$. Confidence interval for θ can be formed in several different ways:

```{r}
id <- 35
set.seed(id)
theta <- id/10
n <- 100
lambda <- 1 / theta

```

### CI length calculation methods

#### 1) Using $\chi_{2n}^2$ distribution via percentiles.

We pass the sample mean $\mathbf{\overline{X}}$ and use the distribution of $2\lambda n \mathbf{\overline{X}}$.

To what what is the distribution of this expression we have to figure out the distribution of $\mathbf{\overline{X}}$.

$\mathbf{\overline{X}} = \frac{\sum{X_i}}{n}$, where $X_i$ is exponential with parameter $\lambda$ The sum of exponentials is Gamma distribution ($Gamma(n, \lambda)$)

$2\lambda Gamma(n, \lambda) \Rightarrow Gamma(n, \frac{1}{2})$ Which is equal to $\chi^2_{2n}$, so we can apply corresponding quantiles and find the confidence interval:

```{r}
chisq_ci <- function(n, theta, sample_mean, alpha) {
  df <- 2 * n
  
  lb <- (2 * n * sample_mean) / qchisq(1 - alpha / 2, df = df)
  ub <- (2 * n * sample_mean) / qchisq(alpha / 2, df = df)
  
  ci_quality <- mean(lb <= theta & theta <= ub)
  ci_max <- max(ub - lb)
  ci_mean <- mean(ub - lb)
  
  return(list(ciq = ci_quality, cix = ci_max, cim = ci_mean))
}


```

#### 2) Using normal approximation for the sample mean

Since we are passing $\alpha$ we consider:

$$
  2\beta - 1 = 1 - \alpha \Rightarrow \beta = 1 - \frac{\alpha}{2}
$$

We find the quantiles for our confidence level and extract $\theta$ from the inequality:

$$
  P(|\theta - \mathbf{\overline{X}} | \leq \frac{z_\beta \theta}{\sqrt{n}}) \Rightarrow - \frac{z_\beta \theta}{\sqrt{n}} \leq \theta - \mathbf{\overline{X}} \leq \frac{z_\beta \theta}{\sqrt{n}} \Rightarrow \mathbf{\overline{X}} - \frac{z_\beta \theta}{\sqrt{n}} \leq \theta  \leq \mathbf{\overline{X}} + \frac{z_\beta \theta}{\sqrt{n}}
$$

```{r}
z_stat <- function(n, theta, sample_mean, alpha) {

  beta <- 1 - alpha / 2
  
  z <- qnorm(beta) * theta / sqrt(n)
  ubs <- sample_mean + z
  lbs <- sample_mean - z
  
  ci_quality <- mean(lbs <= theta & theta <= ubs)
  ci_max <- max(ubs - lbs)
  ci_mean <- mean(ubs - lbs)
  
  return(list(ciq = ci_quality, cix = ci_max, cim = ci_mean))
}

```

#### 3) Confidence interval of confidence level 2β − 1 that is independent of the unknown parameter.

We can play with our inequality to get a confidence interval that has bounds not dependent of $\theta$ $$
  |\theta - \mathbf{\overline{X}}| \leq \frac{z_\beta \theta}{ \sqrt{n}}
$$

$$\Downarrow$$ $$ 
  - \frac{z_\beta \theta}{ \sqrt{n}} \leq 1 - \frac{\mathbf{\overline{X}}}{\theta} \leq \frac{z_\beta \theta}{ \sqrt{n}}
$$ In the next steps we isolate $\theta$ to get the final CI:

$$
\frac{\mathbf{\overline{X}}}{1 + \frac{z_\beta}{ \sqrt{n}}} \leq \theta \leq \frac{\mathbf{\overline{X}}}{1 - \frac{z_\beta}{ \sqrt{n}}}
$$ The function is very similar to the previous one, but with the new bounds.

```{r}
z_stat_indp <- function(n, theta, sample_mean, alpha){
  
  beta <- 1 - alpha / 2
  
  z <- qnorm(beta) / sqrt(n)
  ubs <- sample_mean / (1 - z)
  lbs <- sample_mean / (1 + z)
  
  ci_quality <- mean(lbs <= theta & theta <= ubs)
  ci_max <- max(ubs - lbs)
  ci_mean <- mean(ubs - lbs)
  
  return(list(ciq = ci_quality, cix = ci_max, cim = ci_mean))
}

```

#### 4) Using t-Student distribution

In the previous method of finding the CI we used the population variance of $s^2 = \theta^2$, but if we want a confidence interval that doesn't use the unknown $Var$, we can use the sample standard error (which is the square root of the sample variance) as an approximation ($\sqrt{\frac{S_{\mathbf{XX}}}{n -1}}$). Where $S_{\mathbf{XX}} = \sum(X_i - \theta)^2$ And $n - 1$ - are the degrees of freedom.

Now we form the interval for the standartdized function, using these estimators $$
  \frac{\mathbf{\overline{X}} - \theta}{\sqrt{\frac{S_{\mathbf{XX}}}{n(n -1)}}} \Rightarrow \sqrt{n(n - 1)}\frac{\mathbf{\overline{X}} - \theta}{\sqrt{{S_{\mathbf{XX}}}}}
$$ This expression has Student distribution with $n - 1$ degrees of freedom. Isolating theta from here will give us the bounds $\mathbf{\overline{X}} \pm \frac{t_{\alpha/2}^{(n - 1) \sqrt{S_{\mathbf{XX}}}}}{\sqrt{n(n - 1)}}$

```{r}
t_ci <- function(n, theta, sample_mean, ssd, alpha){
  df <- n - 1
  
  ubs <- sample_mean + (qt(1 - alpha / 2, df) * ssd / sqrt(n)) 
  lbs <- sample_mean - (qt(1 - alpha / 2, df) * ssd / sqrt(n)) 
  
  ci_quality <- mean(lbs <= theta & theta <= ubs)
  ci_max <- max(ubs - lbs)
  ci_mean <- mean(ubs - lbs)
  
  return(list(ciq = ci_quality, cix = ci_max, cim = ci_mean))
}

```

------------------------------------------------------------------------

Here we verify that the confidence intervals of level 1 − α constructed via the four methods of forming them contain the parameter θ = 1/λ approx. 100(1 − α)% of times. Each CI calculation with it's own parameter combinations is stored in the corresponding data frames, from where we could analyse the accuracy of the intervals, their max and mean lengths.

```{r}
#data structures
counter <- 1

sizes <- c(100, 1000, 10000)
repetitions <- c(10, 100, 1000)
alphas <- c(0.1, 0.05, 0.01)
total_rows <- length(sizes) * length(repetitions) * length(alphas)



z_stat_intervals <- data.frame(
  name = "Z-stat",
  n = numeric(total_rows),
  m = numeric(total_rows),
  alpha = numeric(total_rows),
  ci_quality = numeric(total_rows),
  max_len = numeric(total_rows),
  mean_len = numeric(total_rows)
)

chi_sq_intervals <- data.frame(
  name = "Chi-Squared",
  n = numeric(total_rows),
  m = numeric(total_rows),
  alpha = numeric(total_rows),
  ci_quality = numeric(total_rows),
  max_len = numeric(total_rows),
  mean_len = numeric(total_rows)
)

z_indp_intervals <- data.frame(
  name = "Z-stat-independent",
  n = numeric(total_rows),
  m = numeric(total_rows),
  alpha = numeric(total_rows),
  ci_quality = numeric(total_rows),
  max_len = numeric(total_rows),
  mean_len = numeric(total_rows)
)

t_stud_intervals <- data.frame(
  name = "Student t",
  n = numeric(total_rows),
  m = numeric(total_rows),
  alpha = numeric(total_rows),
  ci_quality = numeric(total_rows),
  max_len = numeric(total_rows),
  mean_len = numeric(total_rows)
)

```

```{r}

counter <- 1
for (n in sizes){
  for (m in repetitions){
    for (a in alphas) {
      samples <- matrix(rexp(n * m, rate = lambda), nrow = n)
      
      sample_mean <- colMeans(samples)
      sample_sd <- apply(samples, 2, sd)
      
      c_s <- chisq_ci(n, theta, sample_mean, a)
      z_s <- z_stat(n, theta, sample_mean, a)
      i_s <- z_stat_indp(n, theta, sample_mean, a)
      t_s <- t_ci(n, theta, sample_mean, sample_sd, a)
      
      z_stat_intervals[counter, ] <- list("Z-stat", n, m, a, z_s$ciq, z_s$cix, z_s$cim) 
      chi_sq_intervals[counter, ] <- list("Chi-Squared", n, m, a, c_s$ciq, c_s$cix, c_s$cim)
      z_indp_intervals[counter, ] <- list("Z-stat-independent", n, m, a, i_s$ciq, i_s$cix, i_s$cim) 
      t_stud_intervals[counter, ] <- list("Student t", n, m, a, t_s$ciq, t_s$cix, t_s$cim) 
      counter <- counter + 1
    }
  }

} 




```

### Conclusions from Task 1

First of all, in all of the CI methods different parameters used, such as: sample size (n), number of repetitions (m) and the significance level (alpha). After conducting the calculation we noticed that the longest interval is with the set parameters (n=100, m=1000, alpha=0.01). This is an interesting observation, and we assume it's from the seed. The CI accuracy almost in all cases is very close or coincides with the confidence level.

```{r}
all_intervals <- list(
  z_stat_intervals,
  chi_sq_intervals,
  z_indp_intervals,
  t_stud_intervals
)

for (interval in all_intervals) {
  cat("Interval type:", interval[1, 1], "CI quality on average:", mean(interval[, 5]), "\n")
}
```

From the calculations from these samples we could say that using z-statistic that is independent of the sample variance is the most accurate way for $\theta$. From the first sight we don't really see the difference in the dependence on the $s$ of the Z statistic method, but if you compare the maximum length you could see that the independent method length is 2.62 vs. 1.80 in the other. This gives us more "evidence" to our estimation.

#### Visualisation to support the conclusions

```{r}
library(ggplot2)

results_df <- do.call(rbind, all_intervals)


for (n_val in sizes) {
  
  df_n <- subset(results_df, n == n_val)
  
  print(
    ggplot(df_n, aes(x = alpha, y = mean_len, fill = name)) +
      geom_bar(stat = "identity", position = "dodge") +
      labs(title = paste("Mean CI Length (n =", n_val, ")"),
           x = "Alpha value", y = "Mean CI Length") +
      scale_x_continuous(breaks = c(0.01, 0.05, 0.10)) +
      theme_minimal()
  )
}

```

## Problem 2

Repeat parts (2)–(4) of Problem 1 (with corresponding amendments) for a Poisson distribution P(θ).

$$
X_1,…,X_n∼Poisson(θ)
$$

so

$$
E[X_i​]=θ; Var(X_i)=θ
$$

our sample mean:

$$
\overline{X}=\frac{1}{n}∑X_i 
$$

and:

$$
E(\overline{X}) =(1/n)*n*θ =θ; Var(\overline{X})=(1/n^2)*n*θ=θ/n
$$

#### Method 2 (part 2 of Problem 1)

(using the normal approximation $N(µ, σ_2)$ for $\overline{X}$)

We form Z-statistic and use the fact that it is approximately standard normal:

$$
Z=\frac{\sqrt{​n​}(\overline{X}−θ)}{\sqrt{θ}}∼N(0,1) 
$$

Therefore, for a confidence level of 1−α (with quantile $z_{1−α/2}$) (we use true variance):

$$
P(​|\overline{X}-θ|​≤z_{1−α/2}\sqrt{\frac{​θ}{n}}​​)=P(​\overline{X}-z_{1−α/2}\sqrt{\frac{​θ}{n}}≤θ​≤\overline{X}+z_{1−α/2}\sqrt{\frac{​θ}{n}}​​)=1−α
$$ so

$$
θ∈[\overline{X}−z\sqrt{\frac{θ}{n}}​,\overline{X}+z\sqrt{\frac{θ}{n}}​]
$$

#### Method 3

Start again from $∣\overline{X}−θ∣≤z\sqrt{\frac{θ}{n}}$​​.

This inequality contains θ on both sides (variance is unknown), so steps:

1.  $(\overline{X}−θ)^2≤\frac{z^2θ}{n}$.

2.  $n(\overline{X}^2−2\overline{X}θ +θ^2)-z^2θ≤0$.

3.  $nθ^2- (2\overline{X}n+z^2)θ+n\overline{X}^2≤0$

4.  Solve the quadratic equation and get root$θ1,θ2=\frac{2n\overline{X}+z^2±z\sqrt{4n\overline{X}+z^2}}{2n}$

5.  These two roots give the confidence interval: $CI=[θ1, θ2]$

    This CI does not contain θ inside the formula, so it is usable in practice.

#### Method 4

(get rid of the dependence on θ in (2) is to estimate s via the sample standard error and use approximation of $\overline{X}$ via Student t-distribution)

sample variance:

$$
s^2=\frac{S_{xx}}{n-1}=\frac{1}{n-1}∑(Xi​−\overline{X})^2;
$$

standard error = $\frac{s}{\sqrt{n}}$ = $\sqrt{\frac{S_{XX}}{n(n-1)}}$; then statistic:

$$
T := \frac{(\overline{X}-µ)}{\frac{\sqrt{S_{xx}}}{\sqrt{n(n-1}}} = \frac{\sqrt{n}(\overline{X}-µ)}{s} ∼ t_{n-1}
$$ so,

$$
θ∈[\overline{X}−t_{1-a/2,n-1}\frac{s}{\sqrt{n}}​,\overline{X}+t_{1-a/2,n-1}\frac{s}{\sqrt{n}}]
$$

```{r}
team_id = 35
set.seed(team_id)
theta <- team_id / 10

M <- 1000
sample_sizes <- c(10, 50, 100)
alpha_levels <- c(0.1, 0.05, 0.01)

coverage <- function(lower, upper, theta) {
  mean(lower <= theta & theta <= upper)
}

cat("the number of repetitions: m =", M, "\n")

results_list <- list()
counter <- 1

for (n in sample_sizes) {
  cat("        Sample size n =", n)
  
  x <- matrix(rpois(n * M, lambda = theta), nrow = n)
  sample_mean <- colMeans(x)
  sample_sd <- apply(x, 2, sd)

# Method 2
  for (alpha in alpha_levels) {
    confidence_level <- 1 - alpha
    cat("\nConfidence level =", confidence_level, "\n")

    z <- qnorm(1 - alpha/2)
    lower2 <- sample_mean - z * sqrt(theta / n)
    upper2 <- sample_mean + z * sqrt(theta / n)
    
    cov2 <- coverage(lower2, upper2, theta)
    len2 <- mean(upper2 - lower2)
    max2 <- max(upper2 - lower2)
    
    cat("Method 2:    coverage =", round(cov2,5),
        ", mean length =", round(len2,5), 
        ", max length =", round(max2,5), "\n")
    
    
# Method 3
    a <- 2 * n * sample_mean + z^2
    b <- z * sqrt(4 * n * sample_mean + z^2)
    lower3 <- (a - b) / (2 * n)
    upper3 <- (a + b) / (2 * n)
    
    cov3 <- coverage(lower3, upper3, theta)
    len3 <- mean(upper3 - lower3)
    max3 <- max(upper3 - lower3)
    
    cat("Method 3:    coverage =", round(cov3,5),
        ", mean length =", round(len3,5), 
        ", max length =", round(max3,5), "\n")
    
    
# Method 4    
    t_critical <- qt(1 - alpha/2, df = n - 1)
    lower4 <- sample_mean - t_critical * sample_sd / sqrt(n)
    upper4 <- sample_mean + t_critical * sample_sd / sqrt(n)
    
    cov4 <- coverage(lower4, upper4, theta)
    len4 <- mean(upper4 - lower4)
    max4 <- max(upper4 - lower4)
    
    cat("Method 4:    coverage =", round(cov4,5),
        ", mean length =", round(len4,5), 
        ", max length =", round(max4,5), "\n")
    

    results_list[[counter]] <- data.frame(
      n = n,
      alpha = alpha,
      conf_level = confidence_level,
      Method = rep(c("Method 2", "Method 3", "Method 4"), each = 1),
      Coverage = c(cov2, cov3, cov4),
      Mean_Length = c(len2, len3, len4)
    )
    counter <- counter + 1
  }
  cat("\n")
}
```

#### Visualization

```{r}
library(ggplot2)

results_df <- do.call(rbind, results_list)

for (n_val in sample_sizes) {
  
  df_n <- subset(results_df, n == n_val)
  
  print(
    ggplot(df_n, aes(x = conf_level, y = Coverage,
                     color = Method, group = Method)) +
      geom_line(size = 1.1) +
      geom_point(size = 3) +
      geom_hline(aes(yintercept = conf_level),
                 linetype = "dashed", color = "gray") +
      labs(title = paste("Coverage Probability (n =", n_val, ")"),
           x = "Confidence Level", y = "Coverage") +
      theme_minimal()
  )
  
  print(
    ggplot(df_n, aes(x = conf_level, y = Mean_Length, fill = Method)) +
      geom_bar(stat = "identity", position = "dodge") +
      labs(title = paste("Mean CI Length (n =", n_val, ")"),
           x = "Confidence Level", y = "Mean CI Length") +
      theme_minimal()
  )
}

```

#### Conclusion

**CI coverage**

Across all sample sizes and all confidence levels, the coverage of the intervals is very close to the confidence level. This confirms that all three methods work correctly for estimating the Poisson parameter θ.

-   Method 2 and Method 3 have almost identical coverage for every n and α.

-   Method 4 tends to have slightly better coverage when the sample size is small (n = 10), since the t-distribution compensates for uncertainty in the variance estimate.

**CI precision (mean interval length)**

When comparing the average length of intervals:

-   Method 2 and Method 3 consistently give the shortest intervals

-   Method 4 always has the longest intervals, especially for n = 10, because it uses estimated variance and a t-critical value - quantile of the Student’s t-distribution.

**Also, we can see that as n increases, all methods become more accurate:** average CI length decreases, coverage becomes almost identical across all methods, differences between the methods become smaller.

This matches the expectation: larger n stabilizes the sample mean and reduces uncertainty.

### Task 3

Create dataset:

```{r}
set.seed(42)
n <- 100
mu <- 10
sigma_squared <- 4
sigma <- sqrt(sigma_squared)
dataset <- rnorm(n, mean = mu, sd = sigma)
head(dataset)
## [1] 12.741917 8.870604 10.726257 11.265725 10.808537 9.787751
cat("Population Mean (mu):", mu, "\n")
## Population Mean (mu): 10
cat("Population Variance (sigma_squared):", sigma_squared, "\n")
## Population Variance (sigma_squared): 4
sample_mean <- mean(dataset)
sample_variance <- var(dataset)
cat("Sample Mean:", sample_mean, "\n")
## Sample Mean: 10.06503
cat("Sample Variance:", sample_variance, "\n")
## Sample Variance: 4.337697
```

(a) write the code to find the variance for the dataset

This is a realization for the first common estimator (biased):

$$\sigma_n^2 = \frac{1}{n}\sum_{i=1}^n (X_i - \overline{X})^2$$

```{r}
estimator1 <- function(dataset){
  sample_mean <- mean(dataset)
  count <- 0
  for (el in dataset){
    count <- count + (el - sample_mean)^2
  }
  return (count / length(dataset))
}

```

Now let's write for the second common estimator (unbiased):

$$\sigma_{n-1}^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \overline{X})^2$$

```{r}
estimator2 <- function(dataset){
  sample_mean <- mean(dataset)
  count <- 0
  for (el in dataset){
    count <- count + (el - sample_mean)^2
  }
  return (count / (length(dataset)-1))
}
```

(b) testing estimators on different sizes of datasets

```{r}
set.seed(20)
for (size in c(10, 50, 100, 1000)){
  dataset <- rnorm(size, mean = mu, sd = sigma)
  cat("n =", size, "   estimator1 =", estimator1(dataset),"   estimator2 =", estimator2(dataset), "\n")
}

```

As the sample size increases, both estimators move closer to the true variance $σ^2=4$. The difference between estimator1 and estimator2 also becomes very small. This illustrates that with larger samples both estimators concentrate around the true variance, even though for small $n$ they may deviate a lot just because of randomness.

(c) find the biases

$$\text{Bias}(\sigma_n^2) = E(\sigma_n^2) - \sigma^2$$

$$\text{Bias}(\sigma_{n-1}^2) = E(\sigma_{n-1}^2) - \sigma^2$$

```{r}
bias <- function(n, reps=10000){
  
  estim1 <- numeric(reps)
  estim2 <- numeric(reps)
  
  for (i in 1:reps){
    dataset <- rnorm(n, mean = mu, sd = sigma)
    estim1[i]  <- estimator1(dataset)
    estim2[i] <- estimator2(dataset)
  }
  
  E_sigma_n  <- mean(estim1)
  E_sigma_n1 <- mean(estim2)
  
  bias_n  <- E_sigma_n  - sigma^2
  bias_n1 <- E_sigma_n1 - sigma^2
  
  return(list(
    E_sigma_n  = E_sigma_n,
    E_sigma_n1 = E_sigma_n1,
    bias_n  = bias_n,
    bias_n1 = bias_n1
  ))
}
```

d.  Comment on the results for different values of n

```{r}
set.seed(20)
results_list <- lapply(c(10, 50, 100, 1000), bias)
results_df <- do.call(rbind, lapply(results_list, as.data.frame))
results_df$n <- c(10, 50, 100, 1000)
results_df <- results_df[, c("n", names(results_df)[names(results_df) != "n"])]
results_df
```

Looking at the values in the table, we can see that when the sample size is small (n = 10), both estimators are still quite far from the true variance 4. Estimator1 gives a noticeably smaller value (around 3.61), while estimator2 is closer to 4 but still a bit off because of randomness.

As n increases to 50 and 100, both estimators move much closer to 4, and the biases become smaller. Estimator1 consistently stays below 4, which matches the fact that it is biased downward. Estimator2 is around 4.

For n = 1000, both estimators are extremely close to 4, and the bias is almost zero. At this point the effect of randomness becomes very small, and both estimators show the true value of the population variance.

(e) derive analytically the expected value of each estimator

$$
\mathbb{E}[\sigma_n^2]
=  \mathbb{E}\left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \right]
= \frac{1}{n}(n-1)\sigma^2
= \frac{n-1}{n}\sigma^2
$$

$$
\mathbb{E}[\sigma_{n-1}^2]
= \mathbb{E}\left[ \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2 \right]
= \frac{1}{n-1}(n-1)\sigma^2
= \sigma^2
$$

f.  using the expected values found above, show mathematically what of the above two estimators are unbiased

    for the first estimator

$$
\sigma_n^2 = \frac{1}{n}\sum_{i=1}^n (X_i - \overline{X})^2
$$

$$
\mathbb{E}[\sigma_n^2]
= \frac{1}{n}\,\mathbb{E}\left[\sum_{i=1}^n (X_i - \overline{X})^2\right]
= \frac{1}{n}(n-1)\sigma^2
= \frac{n-1}{n}\sigma^2.
$$

$$
\text{Bias}(\sigma_n^2)
= \mathbb{E}[\sigma_n^2] - \sigma^2
= \frac{n-1}{n}\sigma^2 - \sigma^2
= -\frac{1}{n}\sigma^2.
$$

First estimator is asymptotically unbiased.

for the second estimator

$$
\sigma_{n-1}^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \overline{X})^2
$$

$$
\mathbb{E}[\sigma_{n-1}^2]
= \frac{1}{n-1}(n-1)\sigma^2
= \sigma^2.
$$

$$
\text{Bias}(\sigma_{n-1}^2)
= \mathbb{E}[\sigma_{n-1}^2] - \sigma^2
= \sigma^2 - \sigma^2
= 0.
$$

Second estimator is unbiased.

g\. comment on the results behind theoretical and practical tasks

In general, for small $n$ biased estimator tends to be lower than the actual value, while unbiased fluctuates between lower and upper values.

For large $n$ both estimator go to the actual value, as bias (in the biased estimator) goes to 0.

$$
\sigma_{n-1}^2 \text{ unbiased estimator,}
$$ $$
\sigma_n^2 \text{ asymptotically unbiased } (\sigma_n^2 \xrightarrow{} \sigma^2)
$$
